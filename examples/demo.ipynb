{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ObjectSpace: Object Detection & Tracking Demo\n",
    "\n",
    "This notebook demonstrates the complete pipeline for:\n",
    "1. Processing a video with object detection and tracking\n",
    "2. Evaluating tracking quality without ground truth\n",
    "3. Comparing results across multiple videos\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package if not already installed\n",
    "# !pip install -e ..\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quick Evaluation of Existing Results\n",
    "\n",
    "If you've already processed videos, you can evaluate them instantly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from objectSpace.pipeline import evaluate_annotations\n",
    "\n",
    "# Evaluate existing tracking results\n",
    "result = evaluate_annotations(\n",
    "    \"../output/task3.1_video1_annotations.json\",\n",
    "    print_summary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Access Individual Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"DETAILED METRICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Scores\n",
    "print(f\"\\nüìä SCORES\")\n",
    "print(f\"  Overall Score:    {result.overall_score:.1f}/100\")\n",
    "print(f\"  Continuity Score: {result.continuity_score:.1f}/100\")\n",
    "print(f\"  Stability Score:  {result.stability_score:.1f}/100\")\n",
    "\n",
    "# Track Statistics\n",
    "print(f\"\\nüìà TRACK STATISTICS\")\n",
    "print(f\"  Total Tracks:      {result.tracks.total_tracks}\")\n",
    "print(f\"  Max Concurrent:    {result.tracks.max_concurrent_tracks}\")\n",
    "print(f\"  Avg Active/Frame:  {result.tracks.avg_active_tracks:.1f}\")\n",
    "\n",
    "# Fragmentation\n",
    "print(f\"\\nüîó FRAGMENTATION\")\n",
    "print(f\"  Fragmented Tracks: {result.fragmentation.fragmented_tracks}\")\n",
    "print(f\"  Total Gaps:        {result.fragmentation.total_gaps}\")\n",
    "print(f\"  Avg Gap Length:    {result.fragmentation.avg_gap_length:.1f} frames\")\n",
    "print(f\"  Coverage Ratio:    {result.fragmentation.avg_coverage_ratio:.1%}\")\n",
    "\n",
    "# ID Switches\n",
    "print(f\"\\nüîÑ ID SWITCHES\")\n",
    "print(f\"  Total Switches:    {result.id_switches.total_switches}\")\n",
    "print(f\"  High Confidence:   {result.id_switches.high_confidence_switches}\")\n",
    "print(f\"  Rate (per 100f):   {result.id_switches.switches_per_100_frames:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare Multiple Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from objectSpace.evaluation import TrackingAnalyzer, EvaluationReporter\n",
    "\n",
    "# Find all annotation files\n",
    "output_dir = Path(\"../output\")\n",
    "annotation_files = sorted(output_dir.glob(\"*_annotations.json\"))\n",
    "\n",
    "print(f\"Found {len(annotation_files)} videos to compare\\n\")\n",
    "\n",
    "# Evaluate each\n",
    "analyzer = TrackingAnalyzer()\n",
    "results = []\n",
    "\n",
    "for ann_file in annotation_files:\n",
    "    with open(ann_file) as f:\n",
    "        annotations = json.load(f)\n",
    "    video_name = ann_file.stem.replace(\"_annotations\", \"\")\n",
    "    result = analyzer.analyze(annotations, video_name=video_name)\n",
    "    results.append(result)\n",
    "    print(f\"‚úì {video_name}: {result.overall_score:.1f}/100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison table\n",
    "reporter = EvaluationReporter(use_colors=False)\n",
    "reporter.compare_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Results\n",
    "\n",
    "Display tracked frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample tracked frame\n",
    "tracked_dir = Path(\"../output/task3.1_video1/tracked\")\n",
    "sample_frames = sorted(tracked_dir.glob(\"*.jpg\"))[:5]\n",
    "\n",
    "print(\"Sample tracked frames:\")\n",
    "for frame_path in sample_frames:\n",
    "    print(f\"  {frame_path.name}\")\n",
    "    display(Image(filename=str(frame_path), width=400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export Results to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export evaluation to dictionary\n",
    "evaluation_dict = result.to_dict()\n",
    "\n",
    "print(json.dumps(evaluation_dict, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Process a New Video (Optional)\n",
    "\n",
    "‚ö†Ô∏è **Warning**: This requires significant memory for Mask R-CNN inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to process a new video\n",
    "# from objectSpace.pipeline import DetectionTrackingPipeline\n",
    "#\n",
    "# pipeline = DetectionTrackingPipeline()\n",
    "# results, evaluation = pipeline.process_video_with_evaluation(\n",
    "#     \"../data/videos/task3.1_video1.mp4\",\n",
    "#     output_dir=\"../output/\",\n",
    "#     n_frames=20,  # Reduce for memory\n",
    "#     print_summary=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ‚úÖ **Quick evaluation** of existing tracking results\n",
    "2. ‚úÖ **Detailed metrics** access (fragmentation, ID switches, coverage)\n",
    "3. ‚úÖ **Multi-video comparison** with summary statistics\n",
    "4. ‚úÖ **Visualization** of tracked frames\n",
    "5. ‚úÖ **JSON export** for further analysis\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- Tracker achieves **100% stability** on simple scenes\n",
    "- Performance degrades with scene complexity\n",
    "- Primary bottleneck: **ID association** in crowded scenes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
